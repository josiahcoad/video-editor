# Video Repurposing Pipeline

Turn one long-form video into a full content suite: the YouTube long-form post itself, short-form clips, text posts, and image carousels â€” then schedule everything.

## Environment

```bash
# All scripts are run from the video-editor directory:
cd /Users/apple/Documents/code/marky/marky-app/video-editor

# Standard invocation pattern (edit scripts are in src/edit/):
dotenvx run -f .env -f ../fast-backend/.env -- uv run python src/edit/<script.py> [args]

# For module invocation:
dotenvx run -f .env -f ../fast-backend/.env -- uv run python -m src.edit.<module> [args]

# For scheduling (needs prod Supabase creds):
dotenvx run -f .env -f ../fast-backend/.env -f ../fast-backend/.env.prod -- uv run python src/edit/schedule_post.py [args]

# For locate stage (topic generation):
dotenvx run -f .env -f ../fast-backend/.env -- uv run python -m src.locate.idea_generator [args]
```

**Required env vars:** `DEEPGRAM_API_KEY`, `OPENROUTER_API_KEY`, `GEMINI_API_KEY`, `ELEVENLABS_API_KEY`, `SUPABASE_URL`, `SUPABASE_API_KEY`, `RAPIDAPI_KEY` (for topic research). For b-roll: `PEXELS_API_KEY`. For scheduling: `MARKY_BUSINESS_ID` or `business_id` in project `editing/settings.json` (see Per-project settings).

## Terminology

These terms are used consistently across scripts, docs, and the scheduling pipeline:

| Term | What it is | Where it appears | Example |
|---|---|---|---|
| **Video title** | Short text burned into the first 2-6 seconds of the video as a title card overlay. Search-optimised â€” should read like a real query someone would type into Google or ChatGPT. | `add_title.py`, `suggest_video_title.py` | "how to build trust with niche clients" |
| **Video captions** (subtitles) | Word-by-word text burned into the video throughout, synced to speech. Styled per `settings.json` (font, position, caps). | `add_subtitles.py` | Auto-generated from transcript |
| **Post title** | The `title` field on the Supabase `posts` row. Shown as the post headline in the Marky UI and (where supported) on the social platform. Usually identical to or adapted from the video title. | `schedule_post.py`, `update_post_titles.py` | "how to build trust with niche clients" |
| **Post caption** | The `caption` field on the Supabase `posts` row. The body text that accompanies the video on each social platform. Platform-specific (LinkedIn gets long-form, Twitter gets short, etc.). Generated by `write_copy.py`. | `schedule_post.py`, `write_copy.py` | Platform-tailored copy with hooks, CTAs |

**Key relationships:**
- Video title â†’ burned into the video file (permanent, visual)
- Post title â†’ metadata on the post record (editable, text-only)
- Video captions â†’ burned into the video file (permanent, speech-synced)
- Post caption â†’ metadata on the post record (editable, platform-specific body text)

## Source Code Structure

Scripts are organized by pipeline stage:

```
src/
  profile/                # Stage 1: Brand & buyer profiling
    profile.md            # Onboarding questionnaire template
  locate/                 # Stage 2: Topic research & idea generation
    idea_generator.py     # Demand-driven topic pipeline (ATP API + LLM)
  edit/                   # Stage 3+: Video editing & post-production
    process_video.py      # Main editing orchestrator
    get_transcript.py     # Transcription (Deepgram)
    propose_cuts.py       # LLM-powered segment planning
    apply_cuts.py         # Cut segments from source
    apply_speedup.py      # WPM normalization
    apply_jump_cuts.py    # Dead space removal + zoom cuts
    add_title.py          # Title overlay
    add_subtitles.py      # Caption burn-in
    add_emojis.py         # LLM-picked emoji overlays at key moments
    add_infographic.py   # LLM-planned infographic overlay (Pillow templates)
    add_broll.py         # Pexels b-roll inserts (LLM + transcript â†’ 3 clips Ã— 3s)
    enhance_voice.py      # Podcast-style audio processing
    voice_isolation.py    # ElevenLabs noise removal
    write_copy.py         # Platform-specific copy generation
    schedule_post.py      # Supabase scheduling
    generate_image.py     # Gemini image generation (Nano Banana Pro)
    ...                   # + other editing utilities
```

## Project & Output Directory

**Projects are organized by client, with editing and analysis as sub-areas.** Each client gets a top-level folder under `projects/`. Client-level files (brand profile, topic ideas) live at the client root. Editing-related content lives under `editing/`: voice files at `editing/voices/` and recording sessions at `editing/videos/<YYMMDD-content-name>/` (each with `inputs/`, `outputs/`, and optionally a `settings.json`).

**Convention:** In all examples below, `<client>` refers to `projects/<ClientName>` (e.g. `projects/Josiah`).

### Per-project settings (`settings.json`)

Each **project** has an `editing/settings.json` (e.g. `projects/<ClientName>/editing/settings.json`) that holds editing preferences and the **business ID** for scheduling. Individual sessions can also have a `settings.json` at the session root (alongside `inputs/` and `outputs/`) to override title/caption/speedup for that run. **Most scripts do not read settings directly** â€” the pipeline executor reads them and passes values as CLI flags. The exception is `schedule_post.py`, which **reads `business_id` from project settings** when the video path is under `projects/<Client>/editing/videos/...` (so you don't need to pass `--business_id` or set `MARKY_BUSINESS_ID` per project).

**Business ID:** Set `business_id` in the project's `editing/settings.json`. Example (Josiah/Marky): `"business_id": "bc99888f-39ba-45ec-b7cb-ef9217a60aba"`. Resolution order: `--business_id` flag â†’ project `editing/settings.json` â†’ `MARKY_BUSINESS_ID` env var.

```jsonc
// Example: projects/Anthony/editing/videos/no-credit/settings.json (session overrides)
// Or projects/<ClientName>/editing/settings.json (project-level; include business_id there)
{
  "business_id": "uuid-for-marky-business",   // project-level only; used by schedule_post
  "title": {
    "duration": 6,              // seconds to show title overlay
    "height_percent": 45,       // vertical position (0=bottom, 100=top)
    "anchor": "top",            // which edge height_percent positions
    "style": "classic"          // "classic" (white box) or "dark" (black box)
  },
  "captions": {
    "style": "outline",         // "default" | "classic" | "outline"
    "height_percent": 45,       // vertical position
    "font": "Roboto",           // font family name
    "font_size": 22,            // ASS PlayRes units (20 â‰ˆ 133px on 1920h)
    "word_count": 3,            // max words per subtitle line
    "caps": true                // ALL CAPS captions
  },
  "speedup": {
    "target_wpm": 220           // words per minute target
  },
  "propose_cuts": {
    "duration": 50,             // target seconds per segment (default 50)
    "tolerance": 30             // Â± seconds (default 30). CLI flags override.
  },
  "replacements": {             // spelling/caption fixes (transcription â†’ display)
    "Markey": "Marky",          // common Deepgram mishearings
    "Marquee": "Marky"
  },
  "infographic": {
    "palette": {                // warm/approachable default â€” override per client
      "bg": "#FAF5EF",          // background: warm cream
      "accent": "#D4856B",      // primary: coral/terracotta
      "secondary": "#C4AA8A",   // secondary: tan
      "text": "#3B2F2F",        // body text: dark brown (not black)
      "muted": "#8B7D6B"        // subtle text: muted brown
    },
    "duration": 7               // seconds to display
  },
  "emojis": {
    "max_emojis": 4             // cap on emoji overlay count
  }
}
```

**Style options for captions:**
| Style | Look | Use when |
|-------|------|----------|
| `default` | White text, black outline, semi-transparent dark box | Dark/busy backgrounds |
| `classic` | Black bold text on solid white box | IG/CapCut "Classic" look |
| `outline` | White text, black outline + drop shadow, **no box** | Clean IG Reels look |

```
projects/
  <ClientName>/
    01_definition.md                       # brand profile (Define stage output)
    02_ideas.md                            # topic ideas (Locate stage output)
    brand_profile.json                     # structured profile (for LLM consumption)
    editing/
      voices/                              # per-client voice/style guidelines
        marky_default.md                   # default Marky voice
        marky_video.md                     # video title voice
        marky_youtube.md                   # YouTube-specific guidelines
      music/                               # approved background music library
        preferences.md                     # music preferences (vibe, hard rules, approved/rejected)
        01_track-name_artist.mp3           # pre-downloaded approved tracks
        ...
      videos/
        <YYMMDD-content-name>/             # one folder per recording session
          settings.json                    # session-level settings (title, captions, speedup)
          inputs/
            <source-video>.mp4             # source video(s)
            hook.mov                       # re-recorded hooks, extra takes, etc.
          outputs/
            <stem>-words.json              # full source transcript (Phase 1)
            <stem>-utterances.json
            <stem>-transcript.txt
            cuts.json                      # proposed cuts (Phase 2)
            segment_01/                    # one folder per segment
              01_cut.mp4                   # raw cut
              02_fast.mp4                  # after speedup
              03_jumpcut.mp4               # after jump cuts + dead space removal
              04_enhanced.mp4              # after voice enhancement
              04b_isolated.mp4             # after voice isolation (optional)
              05_titled.mp4                # with title overlay
              06_captioned.mp4             # with captions burned in
              06_captioned.srt             # subtitle file
              06_enhanced.mp4              # voice enhanced again before music (optional but recommended)
              07_music.mp4                 # with background music (final)
            segment_02/
              ...
    analyze/                               # content strategy docs (optional)
      <YYMMDD>-content-strategy.md
```

**Example â€” real project tree:**
```
projects/
  Josiah/
    editing/
      voices/
        marky_default.md
        marky_video.md
        marky_youtube.md
      videos/
        260208-organic-paid/
          inputs/
          outputs/
            merged-words.json
            segment_01/
              01_cut.mp4 â†’ ... â†’ 11_final.mp4
        260209-kai/
          inputs/
          outputs/
            kai-combined-words.json
            segment_01/
              01_cut.mp4 â†’ ... â†’ 06_ffmpeg_enhanced.mp4
  Anthony/
    editing/
      music/
        preferences.md
        01_be-the-change_alumo.mp3
        02_facetoface_florens.mp3
      videos/
        no-credit/
          settings.json
          inputs/
          outputs/
    analyze/
      260209-content-strategy.md
  HunterZier/
    01_definition.md
    02_ideas.md
  Laurie/
    laurie_foster.md
    editing/
      videos/
        260208-laurie/
          inputs/
          outputs/
```

Create the project directories at the start of the pipeline:
```bash
# New client:
mkdir -p projects/<ClientName>/editing/{voices,videos,music}

# New recording session:
mkdir -p projects/<ClientName>/editing/videos/<YYMMDD-content-name>/{inputs,outputs/segment_01}
```

Source videos and re-recorded hooks go in `inputs/`. All pipeline outputs go in `outputs/` (source-level transcripts) or `outputs/segment_XX/` (segment-level files). Do NOT place output files in `inputs/` or outside the session directory.

### Using project settings in the pipeline

**At the start of every pipeline run**, check whether the session has a `settings.json`:

```bash
cat <client>/editing/videos/<session>/settings.json
```

If it exists, **read the values and pass them as explicit CLI flags** to the editing scripts. For example, if `settings.json` has `title.duration: 6` and `captions.style: "outline"`, you pass `--duration 6` to `add_title.py` and `--style outline` to `add_subtitles.py`.

Scripts are standalone tools driven entirely by flags â€” they don't require a settings file. The settings file is a reference for the pipeline executor (human or agent) to know the project's preferences. If no `settings.json` exists, use reasonable defaults and ask the user.

---

## Phase 0: Download Source Video (if YouTube)

If the source is a YouTube URL, download it using `yt-dlp` from the fast-backend virtualenv:

```bash
cd /Users/apple/Documents/code/marky/marky-app/fast-backend
.venv/bin/yt-dlp --cookies-from-browser safari \
  -f "bestvideo[ext=mp4][height<=1080]+bestaudio[ext=m4a]/best[ext=mp4]/best" \
  -o "/Users/apple/Documents/code/marky/marky-app/video-editor/%(title)s.%(ext)s" \
  "<youtube-url>"
```

- **`--cookies-from-browser safari`** is usually required â€” YouTube blocks raw downloads with 403 errors without it.
- If downloads still fail, update yt-dlp first: `uv pip install --upgrade yt-dlp` (the fast-backend venv). YouTube frequently changes its anti-bot measures and older versions stop working.
- `yt-dlp` is installed in the fast-backend virtualenv, not video-editor. Always use `.venv/bin/yt-dlp` from the fast-backend directory.

---

## Phase 1: Transcribe

```bash
python src/edit/get_transcript.py <video.mp4>
```

By default, transcript files are written next to the source video. **Move or copy them into `outputs/<video-stem>/`** to keep everything organized.

Produces three files:
- `<stem>-words.json` â€” word-level timestamps (used by propose_cuts and add_subtitles)
- `<stem>-utterances.json` â€” sentence-level groupings
- `<stem>-transcript.txt` â€” plain text

**Do this first.** Many downstream scripts can transcribe on their own. For long-form source videos, pass existing transcripts via `--transcript` to avoid redundant API calls. For short-form segments (< 90s), letting scripts auto-transcribe is fine â€” it's cheap (~$0.01) and ensures timestamps match the actual video timeline (important after cuts or speedup).

---

## Phase 2: Propose Cuts

```bash
python src/edit/propose_cuts.py --transcript <stem>-words.json
```

- Outputs a JSON array of segment plans to stdout. Capture it.
- Each segment has: number, summary, sections (with start/end timestamps), estimated duration.
- `--duration` is the target length per segment (default 50). `--tolerance` is Â± seconds (default 30).
- `--count N` forces exactly N segments. Without it, the LLM decides based on content quality.
- `--prompt` is **optional backup only** â€” use it when the video's content is ambiguous and the LLM needs a nudge on what to focus on. The script's built-in system prompt handles cold opens, setup-payoff consistency, phrase completeness, narrative coherence, and section consolidation automatically. Don't pass `--prompt` by default.

### Quality control (MANDATORY â€” stop and wait for human approval)

After running `propose_cuts`, **always stop and present a critical review to the user before proceeding.** Do not move to Phase 3 until the user approves.

**What to do:**

1. Read the proposed cuts output carefully.
2. Read the full transcript alongside the cuts to understand what was kept and what was dropped.
3. Rate the cuts honestly (e.g. 6/10, 8/10) compared to what you would have done as an experienced editor. Consider:
   - **Tightness:** Are the segments lean, or did the LLM leave in filler/verbose stretches it should have cut?
   - **Hook quality:** Does each segment open strong, or does it start with a weak/generic moment?
   - **Segment opener coherence:** Read the FIRST 5-10 words of each segment aloud. Do they form a complete, self-contained statement? If the opener is a sentence fragment (e.g. "paid ads, social media ads, and organic" without the lead-in "I wanted to talk about the difference between..."), the cut is broken â€” a cold viewer's first impression will be confusion. Either extend the cut to include the full sentence or find a different start point that begins a new thought.
   - **No continuation-word openers:** The first words of every segment (and of every section within a segment, e.g. the body after a hook) must NOT be a conjunction or transition: "Also", "So", "And", "But", "Well", "Then", "However", etc. If the body starts with "Also, there are...", the cut is wrong â€” move the section start so it begins with "There are...". This is a mandatory QC item; fix cuts and re-run from apply_cuts if you find it.
   - **No duplicate phrases:** The same sentence or near-identical line must NOT appear twice in one segment (e.g. hook says "Everybody thinks you need a perfect credit score to buy a house" and body repeats "So a lot of people think you need a perfect credit score to buy a house"). If you hear the same idea twice, the cuts are wrong â€” trim the body start to after the duplicate so the viewer only hears it once. This is a mandatory QC item; fix cuts and re-run from apply_cuts if you find it.
   - **Section granularity:** Did the LLM use enough short sections for good pacing, or is it just keeping big continuous stretches with obvious junk removed?
   - **Duration:** Is each segment the right length for its content, or is it padded/rushed?
   - **Cold viewer test:** Would each segment make sense and hook someone with zero context?
   - **Creative decisions:** Did the LLM make any smart editorial choices (cold opens, reordering for impact), or did it just mechanically remove bad parts?
4. Tell the user your rating and explain specifically what you'd change.
5. **Wait for the user's decision:**
   - **Ship it** â€” proceed to Phase 3 with the current cuts.
   - **Iterate** â€” re-run `propose_cuts` with adjusted parameters (different `--duration`, `--tolerance`, `--count`, or `--model`) to improve the result. Use `--prompt` only as a last resort when the LLM consistently misunderstands the content focus. Then re-review.

**Do NOT silently accept mediocre cuts.** The propose_cuts step is the most consequential decision in the pipeline â€” everything downstream (titles, captions, music) is wasted if the underlying cuts are weak.

### General quality guidelines

- **Review the output.** Check that segments are 25-55s (within tolerance). Reject anything under 20s.
- The LLM sometimes splits related sub-topics into separate segments â€” merge them if they belong together.
- A good segment has a hook, a body, and a strong ending. If one feels like a fragment, it probably is.
- The LLM may reuse the video's intro as a hook for multiple segments. That's fine for 1-2 segments, but not all of them.
- **Prioritize quality over quantity.** 3-5 great shorts > 10 mediocre ones.

### When NOT to split into multiple segments

The LLM is biased toward producing multiple segments even when the content doesn't warrant it. Watch for these signs:

- **Single linear procedure** (e.g., a setup guide, a recipe, a walkthrough): Splitting a step-by-step process into 3 parts produces 3 fragments, not 3 shorts. Segments 2 and 3 will lack context and won't hook a cold viewer. Prefer one condensed short covering the full flow, or just post the whole video (TikTok supports up to 10 minutes).
- **Source is already short** (under 5 minutes): There may not be enough substance to split. Consider trimming filler and posting as a single short.
- **Each proposed segment fails the "cold viewer" test:** Would someone landing on THIS segment with zero context understand what's happening and want to keep watching? If not, it's a fragment, not a standalone short.
- **The segments are just chronological slices:** If segment 1 = first third, segment 2 = middle third, segment 3 = last third, the LLM didn't do any creative editorial work â€” it just divided by time.

**Good source material for multi-segment splits:** Multi-topic discussions, interviews with distinct questions, listicle-style content ("5 things I learned"), debates with clear position changes.

---

## Phase 2b: Cut Text Verification (MANDATORY â€” before touching ffmpeg)

After proposing cuts (whether from `propose_cuts.py`, manual selection, or any other method), **always preview the actual transcript text for each timestamp range before cutting**. This catches mid-sentence cuts, trailing conjunctions, and stray words from adjacent sentences that the LLM (or human) missed.

```python
# Quick verification script â€” run for every proposed cut
import json
with open('<stem>-words.json') as f:
    words = json.load(f)

def text_for_range(start, end):
    wds = [w for w in words if start <= w['start'] <= end]
    return ' '.join(w['word'] for w in wds)

# For each segment, print what the viewer will actually hear:
for label, ranges in proposed_cuts.items():
    print(f"\n=== {label} ===")
    for start, end in ranges:
        text = text_for_range(start, end)
        print(f"  [{start:.2f}-{end:.2f}]: \"{text}\"")
```

**What to check:**

1. **First words of each segment (and of each section):** Do they form a complete, self-contained statement? If the first words are "a product, I, like, probably a lot of tech bros" instead of "when it came to building a product", the cut starts mid-sentence. Back up the start timestamp to the sentence/clause boundary. **Explicit check:** No segment/section may start with a continuation word ("Also", "So", "And", "But", "Well", "Then", "However"). If the body of a segment (e.g. after a hook) starts with "Also, there are...", move the section start to "There are..." and re-run from apply_cuts for that segment.
2. **No duplicate phrases:** Read the full text of the segment (all sections concatenated). Does the same sentence or near-identical line appear twice (e.g. hook and body both saying "everyone thinks you need a perfect credit score to buy a house")? If yes, trim the later section's start so the duplicate is removed â€” the viewer must only hear the idea once.
3. **Last words of each section**: Do they end on a stray conjunction ("and", "but", "so")? Trim the end timestamp to before the trailing word.
4. **Cold open text**: Does it include the full punchline? If it ends at "I can now say" instead of "I can now say confidently that I am 100% committed to helping people sell more through social media", extend the end timestamp.
5. **Floating point edge cases**: Word-level timestamps can have precision issues. If a word at exactly `1526.27` is missing when your range starts at `1526.27`, back up by 0.10s (`1526.17`).

**Do NOT proceed to Phase 3 until every segment's text reads as clean, natural speech that a cold viewer would understand from word one.**

> **Root cause this prevents:** Picking timestamps from word-chunk boundaries or 15-word groups without verifying they land on sentence boundaries. The downstream pipeline (speedup, jump cuts) cannot fix a cut that starts mid-sentence â€” it only makes the audio faster and adds zoom effects. Garbage in, garbage out.

---

## Phase 3: Cut Segments

```bash
python src/edit/apply_cuts.py <video.mp4> <output_segment.mp4> --cuts "0.8:7.06,27.54:43.81"
```

- `--cuts` takes comma-separated `start:end` timestamp ranges (in seconds).
- These come directly from the propose_cuts output (each segment's sections).
- The script concatenates the specified ranges into one continuous clip.
- **Add 0.15-0.2s buffer before the first word's start timestamp.** The LLM proposes cuts at exact word boundaries, but plosive consonants (p, t, k, b, d, g) start 50-100ms before the transcription timestamp. Without buffer, the first syllable gets partially swallowed â€” especially after speedup compresses the margin further. Example: if the first word starts at 12.08s, use `11.90:...` not `12.08:...`.

---

## Phase 3b: Speedup to 220 WPM (optional)

**You can skip this step** and run jump cuts (Phase 3c) directly on the cut segment â€” jump cuts alone often provide enough pacing. Use speedup when you want a stricter 220 WPM target on top of dead-space removal.

If you do use speedup, run it **before** jump cuts because:
- The 0.4s gap threshold should apply at the **final playback pace**, not the original pace
- The 5s max cut duration should match the **viewer's experience**, not the raw timing
- If you jump-cut first then speed up, remaining 0.4s gaps compress to ~0.3s (tighter than intended) and 5s segments shrink to ~3.8s (more jarring than intended)

```bash
python src/edit/apply_speedup.py <segment.mp4> <segment-fast.mp4> --wpm 220
```

- Transcribes the video, calculates current WPM, derives the speedup multiplier.
- If already at/above 220 WPM, no speedup is applied.

---

## Phase 3c: Jump Cuts + Dead Space Removal

Aggressive post-processing to create punchy, fast-paced short-form content. Combines dead space removal and artificial zoom-based jump cuts in a single pass. **Run on the cut segment from Phase 3 (or on the sped-up file if you ran Phase 3b).**

**Goal:** No more than **0.4s** of dead time between words. No continuous shot longer than **5 seconds** without a visual cut.

```bash
# Let the script auto-transcribe the input video. This is the simplest
# and most reliable approach â€” timestamps match the actual video exactly.
#
# The script auto-discovers 01_cut.boundaries.json (written by
# apply_cuts.py) in the same directory and suppresses zoom toggles
# near content-cut join points to avoid "double jumps".
python src/edit/apply_jump_cuts.py <segment-fast.mp4> <segment-jumpcut.mp4> \
  --gap-threshold 0.4 --max-cut-duration 5.0 --zoom-factor 1.2
```

**Why auto-transcribe instead of passing the original transcript?** Transcribing a 30-60s short costs ~$0.01 and gives you timestamps that exactly match the video timeline. The old approach passed `--transcript <original> --speedup <factor>`, which required complex timestamp remapping for cut videos (where the input is a concatenation of non-contiguous source ranges). At our typical 1.2-1.5x speedups, Deepgram still detects pauses reliably (a 0.4s gap becomes ~0.3s, well above Deepgram's ~40ms resolution). The gap-destruction concern only matters at extreme 2x+ speedups, which we don't use.

> **Legacy fallback:** If you do pass `--transcript` and `--speedup`, the script still supports the old path and will remap timestamps using `source_ranges` from the boundaries sidecar. But prefer auto-transcription â€” it's simpler and avoids coupling between scripts.

**What it does:**
1. Auto-transcribes the input video (or uses the provided transcript as a fallback)
2. Removes all gaps > 0.4s between spoken words (creates natural jump cuts)
3. For remaining continuous sections > 5s, finds the most natural split point using a weighted score: gap duration (80%) + proximity to segment midpoint (20%). Gap-dominant scoring finds natural pauses; balance prevents lopsided splits
4. Alternates between normal and zoomed-in framing to simulate camera cuts
5. **Suppresses zoom toggles within Â±1s of content-cut boundaries** (from `apply_cuts`) to prevent "double jumps" where a content discontinuity and a zoom change happen simultaneously
6. Pads 0.12s before the first word and 0.25s after the last word to preserve consonant onsets and allow the final frame to resolve naturally

**V1 (current):** Zoom is a uniform 1.2x crop biased toward the top half of the frame (where the face usually is).

**Future v2 â€” smart face-aware zoom:**
- Use OpenCV face detection with adaptive sampling: start at 10s intervals, if face moves > threshold, subdivide (5s â†’ 2.5s â†’ 1s â†’ 0.5s â†’ 0.1s) to track movement precisely
- Center crop on detected face position per segment
- Apply rule-of-thirds positioning for cinematic framing
- Smooth pan between face positions (Ken Burns effect)

---

## Phase 3d: Hook Recording + Merge

After the segment is cut, tightened, and sped up, propose hooks and optionally prepend a re-recorded hook.

### Step 1: Propose hooks

```bash
python src/edit/propose_hooks.py --transcript <words.json> --voice <client>/editing/voices/marky_video.md
```

- Generates two categories: **cold opens** (soundbites from the video with timestamps) and **written hooks** (original lines to deliver on camera).
- Present the top options to the user and let them pick.

### Step 2: User records the hook

- The user records a short (3-8 second) video delivering the chosen hook.
- Place the recording in the session's `inputs/` folder (e.g. `<client>/editing/videos/<session>/inputs/`).

### Step 3: Cut the hook

The recording will likely contain false starts, multiple takes, or dead time. Extract only the clean take:

1. Transcribe the hook recording to find the good take's timestamps.
2. Extract just that take, cropping to 9:16 if the recording is landscape:

```bash
ffmpeg -y -i inputs/hook.mov -ss <start> -to <end> \
  -vf "crop=ih*9/16:ih" -c:v h264_videotoolbox -b:v 8M \
  -c:a aac -b:a 192k outputs/segment_XX/04_hook_raw.mp4
```

### Step 3b: Speed up the hook

**The hook MUST be sped up to match the main segment's target WPM (e.g. 220).** A pacing mismatch between hook and main segment is jarring â€” the viewer feels the tempo shift.

```bash
python src/edit/apply_speedup.py outputs/segment_XX/04_hook_raw.mp4 /tmp/hook_fast.mp4 --wpm 220
```

### Step 3c: Trim the hook tightly

After speedup, trim trailing silence so the join with the main segment is seamless. Transcribe the sped-up hook to find where the last word ends, then cut to `last_word_end + 0.06s`:

```bash
# Find last word end time from transcript, then:
ffmpeg -y -i /tmp/hook_fast.mp4 -t <last_word_end + 0.06> \
  -c:v h264_videotoolbox -b:v 8M -c:a aac -b:a 192k \
  outputs/segment_XX/04_hook.mp4
```

**Why 0.06s buffer?** The word-level transcript reports the end of the vowel sound, but trailing consonants and natural voice decay extend slightly beyond that. Cutting tighter than ~0.05s sounds unnatural; leaving more than ~0.10s creates a perceptible gap at the join.

### Step 4: Compose hook + segment

**Always use ffmpeg `filter_complex` concat** with `fps=30` normalization â€” never stream copy (`-c copy` or `compose_clips.py` fast mode). The hook and main segment will almost always have different framerates after speedup, and stream copy concat silently produces incorrect durations.

```bash
ffmpeg -y \
  -i outputs/segment_XX/04_hook.mp4 \
  -i outputs/segment_XX/03_fast.mp4 \
  -filter_complex "[0:v]fps=30[v0];[1:v]fps=30[v1];[v0][0:a][v1][1:a]concat=n=2:v=1:a=1[vout][aout]" \
  -map "[vout]" -map "[aout]" \
  -c:v h264_videotoolbox -b:v 8M -c:a aac -b:a 192k \
  outputs/segment_XX/05_hooked.mp4
```

- **Use the hooked version for all subsequent phases** (title, captions, etc.).

### When to skip

- If none of the proposed hooks are strong enough (score < 7)
- If the video's natural opening is already a strong hook
- If the user doesn't want to re-record â€” fall back to a cold open (pulled from the existing video) or skip the hook entirely

---

## Phase 4: Voice Enhancement

**Always run this step.** Applies a podcast-style audio chain that makes iPhone/webcam recordings sound closer to a professional mic: bass warmth, harshness reduction, dynamic compression, and loudness normalization. Free and instant (ffmpeg only, no API).

```bash
python src/edit/enhance_voice.py <segment.mp4> <segment-enhanced.mp4>
```

- Boosts bass at 120Hz/220Hz for warmth and fullness
- Cuts harsh midrange (3kHz) and sibilance (6.5kHz) to reduce tinny/echo quality
- Compresses dynamics for a broadcast/podcast feel
- Normalizes loudness to -16 LUFS (podcast standard)
- Video stream is copied (no re-encode) â€” only audio is processed

**Run this BEFORE title and captions.** Voice enhancement changes the audio, and the caption transcript must come from the final audio. Pipeline order: jump cuts â†’ enhance â†’ (isolate) â†’ title â†’ transcribe â†’ captions.

### Phase 4 alt: Voice Isolation (optional â€” only when requested)

If the audio is noticeably noisy (background music, construction, wind), use ElevenLabs voice isolation **after** enhancement:

```bash
python src/edit/voice_isolation.py <enhanced.mp4> <isolated.mp4>
```

- Uses ElevenLabs Audio Isolation API (costs API credits).
- Removes background noise, hum, room reverb.
- More aggressive than the ffmpeg chain â€” strips everything that isn't voice.
- If adding music later, voice isolation helps the voice stay clean against the music track.
- Run this **before** title/captions â€” same reason as enhancement: the transcript must match the final audio.

---

## Phase 4b: Crop to 9:16 Portrait (for short-form)

If the source video is 16:9 landscape and the target is TikTok/Reels/Shorts, crop to 9:16 **before** adding any visual overlays (titles, captions). Overlays burned into a 16:9 frame will be clipped or mis-positioned after cropping.

```bash
ffmpeg -i <segment-isolated.mp4> -vf "crop=ih*9/16:ih" \
  -c:v h264_videotoolbox -c:a copy <segment-portrait.mp4> -y
```

- This center-crops to 9:16, keeping the full height and taking the middle vertical slice.
- Works well when the speaker is centered. If the speaker is off-center, you'll need to adjust the crop offset (add `:x_offset:0` to the crop filter).
- **All subsequent steps (check_placement, add_title, add_subtitles) must run on the cropped version.** Their placement values will be completely different for 9:16 vs 16:9.

---

## Phase 5: Title Overlay

### Step 5a: Determine placement

```bash
python src/edit/check_placement.py <segment-isolated.mp4>
```

**Run this PER video.** Different video layouts need different placements:

| Layout | Typical title height | Notes |
|--------|---------------------|-------|
| Full-screen talking head | `--height 20 --anchor bottom` | Face is centered/upper, title sits below torso |
| Split-screen (graph + face) | `--height 15 --anchor bottom` | Face is in lower half, less room to work with |
| Split-screen screen demo | `--anchor center --dark` | Title in the **middle of the frame** (centered vertically). Use for screen-share + face layouts so the title doesn't cover the whole top half. `--dark` gives black bg + white text to contrast with screen content. |

The script outputs `TITLE_HEIGHT_PERCENT`, `TITLE_ANCHOR`, `CAPTION_HEIGHT_PERCENT`, and reasoning.

**DO NOT use a fixed height for all videos.** The split-screen layout caught us â€” `--height 20` put the title right on the speaker's face because the face was much lower in the frame than expected.

**Split-screen screen demos:** For product demo videos where the top half is a screen recording and the bottom half is the speaker, put the title **in the middle of the screen** (`--anchor center --dark`) so it sits between the screen area and the face without covering either. Use a black background with white text (`--dark`) to contrast with the screen content.

### Step 5b: Apply title

```bash
# Standard (white box, black text â€” use values from settings.json if available):
python src/edit/add_title.py <segment.mp4> "Title Text Here" <output.mp4> \
  --duration 6 --height <N> --anchor bottom --voice <client>/editing/voices/marky_default.md

# Split-screen screen demo (black box, white text â€” title centered in frame):
python src/edit/add_title.py <segment.mp4> "Title Text Here" <output.mp4> \
  --duration 6 --anchor center --dark --voice <client>/editing/voices/marky_default.md
```

- Read `settings.json` â†’ `title.duration`, `title.height_percent`, `title.anchor` and pass as flags.
- `--duration 6` â€” show the title for 6 seconds (not 20 â€” that's too long for a short).
- Default: white rounded rectangle with black bold text.
- `--dark` â€” black rounded rectangle with white bold text. Use for split-screen screen demos where the title is centered and needs to contrast with bright screen content.
- If no title text is provided, the script will auto-generate one from the transcript via LLM.

### Title constraints

- **Platform chrome:** TikTok/Instagram/YouTube Shorts all have UI overlays in the bottom ~10% of the frame. Never place the title there (i.e., don't go below `--height 8` or so).
- **Split-screen trade-off:** With split-screen content, the face occupies the lower half. You're stuck between overlapping the face (~15%) or getting clipped by platform chrome (~0-8%). 15% is the least-bad option.
- Title text is auto-uppercased. Keep titles concise (aim for 2 lines max at 30 chars/line).

### Title QC (MANDATORY â€” visual inspection)

After adding the title, **run `preview_chrome.py` at ~1-2s, then view the generated image** (the agent can read image files). Do not delegate this to the user. Check:
- Title is NOT overlapping the speaker's face
- Title is NOT clipped by platform chrome (bottom ~10%)
- Text is readable (not too small, not too many lines)
- Title text is accurate and concise

If the title placement or rendering looks bad, **tell the user what's wrong and suggest iterating on the `add_title.py` script** to fix the underlying issue â€” don't just re-run with different params if the script itself is producing poor results.

---

## Phase 6: Captions (Subtitles)

**CRITICAL: Transcribe the titled video first.** The caption transcript must come from the video being captioned (or an audio-identical ancestor). Using a transcript from before jump cuts will produce out-of-sync captions because dead space removal shifts all timestamps.

```bash
# Step 1: Transcribe the titled video (the one captions will be burned onto):
python src/edit/get_transcript.py <titled.mp4>

# Step 2: Apply captions using THAT transcript:
python src/edit/add_subtitles.py <titled.mp4> \
  --output <captioned.mp4> \
  --transcript <titled>-words.json \
  --height 12 --style outline --font-size 16 --caps --delay 6 \
  --replace "Markey:Marky"
```

- Read `settings.json` â†’ `captions.style`, `captions.height_percent`, `captions.font_size`, `captions.caps`, `title.duration` (for `--delay`), and `replacements` (or pass `--settings <path>` so the script loads them). Pass as flags.
- `--height` sets the caption vertical position (0=bottom, 100=top). **Default is 12%** (captions sit further below the face). Lower value = closer to bottom of frame. Use the value from `check_placement.py` (`CAPTION_HEIGHT_PERCENT`) if overriding.
- `--style` â€” caption visual preset: `default` (white on dark box), `classic` (black on white box), `outline` (white text + outline + shadow, no box).
- `--caps` / `--no-caps` â€” force ALL CAPS or sentence case.
- `--delay` â€” suppress captions for the first N seconds (use `title.duration` so captions don't overlap the title).
- `--transcript` â€” path to the word-level transcript JSON **from the titled video**. The script auto-discovers the sibling `*-utterances.json` for smart captioning (splits at punctuation).
- `--replace` fixes common transcription errors. Format: `"wrong1:right1,wrong2:right2"`. Alternatively pass `--settings <path>` to use `settings.replacements` from the project/session JSON (e.g. `{"Markey": "Marky", "Marquee": "Marky"}`). Both can be used; replacements are merged.
- Captions are burned in as styled ASS subtitles (3 words per line by default, fewer at punctuation breaks).

### Caption placement guidelines

- **Default:** `height_percent` defaults to **12%** (captions ~1/5 lower than previous 15%), so they sit further below the face without overlapping chin/jaw.
- Lower `--height` = captions closer to bottom of frame (further below face). Higher = closer to center.

| Layout | Caption height | Why |
|--------|---------------|-----|
| Full-screen talking head | **12% (default)** or 15â€“30% | Default 12% keeps captions well below face. Use 15â€“30% if you need them slightly higher (e.g. chest level). |
| Split-screen | 45% | Sits in the gap between graph and face |

**IMPORTANT:** 43% was an old recommendation and often overlapped the speaker's chin/jaw. **12% default** or **30%** keeps captions below the face. If the speaker is unusually tall in the frame, stay at 12% or lower.

**Known issue:** In split-screen layouts, captions at 45% can still overlap the forehead during speaking sections. There's no perfect solution â€” the face and the content compete for the same vertical space. This is a content/framing issue, not a tools issue.

### Caption QC (MANDATORY â€” visual inspection)

After burning in captions, **run `preview_chrome.py` at tâ‰ˆ4s and tâ‰ˆ10s, then view both output images** (the agent can read image files). Do not delegate this to the user. Verify:
1. **tâ‰ˆ4s** â€” title + captions both visible. Check they don't overlap each other or the face, and are above platform chrome.
2. **tâ‰ˆ10s** â€” captions only (title has faded). Check captions are clear, well-positioned, and not in the chrome zone.

**Face-overlap check (the #1 caption failure):**
- Look at the extracted frames. Are the captions covering ANY part of the speaker's face â€” chin, jaw, mouth, or neck?
- The captions must sit on the **chest or below**, never touching the face. If you can draw a horizontal line at the bottom of the chin and the captions are above that line, they are TOO HIGH.
- If captions overlap the face, **lower `--height` by 5-10%** and re-render. Do not accept face-overlapping captions.
- Common mistake: defaulting to `--height 43` for talking heads. The correct value is **30%** (see table above).

Evaluate the caption rendering quality critically:
- Are the captions styled well (font size, weight, outline, readability)?
- Is the word timing accurate (words appear when spoken)?
- Are word replacements applied correctly (e.g. "Marquee" â†’ "Marky")?
- Is the overall look professional or does it feel amateur?

If the captions look bad â€” poor styling, wrong positioning, timing issues, ugly rendering â€” **tell the user what's wrong and suggest iterating on the `add_subtitles.py` script** to improve the output quality. Don't just accept mediocre results.

---

## Phase 6b: Emoji Overlays (optional â€” when content benefits from visual emphasis)

Add emoji overlays at key spoken moments to reinforce concepts and add visual energy. Uses an LLM to analyze the transcript and pick the best 3-5 moments, then renders emoji PNGs with white circular backings and composites them via ffmpeg.

```bash
# Dry-run first (recommended) â€” review picks before committing to a render:
python -m src.edit.add_emojis <captioned.mp4> <emojis.mp4> \
  --transcript <segment-words.json> --dry-run

# Full render:
python -m src.edit.add_emojis <captioned.mp4> <emojis.mp4> \
  --transcript <segment-words.json> --max-emojis 4
```

- `--dry-run` prints the LLM's emoji picks (timestamp, emoji, position, rationale) as JSON to stdout without rendering. Use this to review and sanity-check before a full encode.
- `--max-emojis N` caps the number of overlays (default 4). More than 5 feels cluttered.
- `--transcript` accepts a word-level JSON. If omitted, the script transcribes the video automatically (wasteful if you already have one â€” always pass it).
- Emojis are 240Ã—240px with a semi-transparent white circular backing for visibility against any background.
- Positions alternate between top-left and top-right for visual variety.
- The LLM avoids the first 2s (title card) and last 3s (outro), and spaces emojis at least 5s apart.

### When to use

- **Talking-head explainers** where the speaker mentions concrete nouns, numbers, or strong concepts â€” emojis anchor the idea visually.
- **Short-form content** where visual variety keeps attention â€” emojis act as mini-cuts without needing b-roll.

### When to skip

- **Screen recordings / product demos** â€” the screen content IS the visual. Emojis would distract.
- **Emotional / serious content** â€” emojis undercut gravity.
- **Videos that already have infographics or heavy overlays** â€” adding emojis on top feels cluttered. Pick one or the other.

### QC

- **Dry-run review:** Read each pick's rationale. Does the emoji genuinely reinforce the spoken word, or is it forced? Reject picks that are generic (e.g., ðŸ”¥ on a non-emphatic moment).
- **After render:** Scrub through and verify each emoji appears at the right moment, doesn't overlap captions/title, and disappears cleanly.

---

## Phase 6c: Infographic Overlay (optional â€” when a concept benefits from visual explanation)

Insert a full-screen infographic at the moment the speaker discusses a concept that's hard to grasp from audio alone. Uses an LLM to pick the concept, choose a template, and determine timing, then renders the graphic with Pillow and overlays it via ffmpeg.

```bash
# Dry-run â€” review concept, template, timing, and preview image:
python -m src.edit.add_infographic <captioned.mp4> <infographic.mp4> \
  --transcript <segment-words.json> --dry-run

# Full render (default 7s display):
python -m src.edit.add_infographic <captioned.mp4> <infographic.mp4> \
  --transcript <segment-words.json> --duration 7

# With custom palette from settings.json:
python -m src.edit.add_infographic <captioned.mp4> <infographic.mp4> \
  --transcript <segment-words.json> \
  --palette '{"bg": "#FAF5EF", "accent": "#D4856B", "secondary": "#C4AA8A", "text": "#3B2F2F", "muted": "#8B7D6B"}'
```

- `--dry-run` saves a preview PNG to `/tmp/infographic_preview.png` and prints the plan as JSON. **Always dry-run first** â€” inspect the image and verify the concept/timing make sense before a full encode.
- `--duration N` sets how long the infographic displays (default 7s). Should match how long the speaker discusses the concept.
- `--template` forces a specific template if the LLM picks wrong. Options: `funnel`, `compare`, `list`, `bigstat`.
- `--palette` overrides the default warm cream/coral/tan palette. Pass a JSON object with any subset of keys: `bg`, `accent`, `secondary`, `text`, `muted`, `white`.

### Available templates

| Template | Best for | Example |
|----------|----------|---------|
| `funnel` | Processes, pipelines, causeâ†’effect flows | Paid Ads â†’ Awareness â†’ Trust â†’ Purchase |
| `compare` | X vs Y, side-by-side contrasts | High Ticket vs Low Ticket |
| `list` | Key points, tips, features (3-5 items) | "5 things every founder needs" |
| `bigstat` | Dramatic number, percentage, keyword | "73% of buyers..." |

### When to use

- **Explainers** where the speaker describes an abstract concept (trust gap, sales funnel, customer journey) â€” the graphic anchors the idea.
- **Comparisons** where the speaker contrasts two approaches â€” seeing them side-by-side is clearer than hearing them sequentially.
- **Key statistics** that are more impactful when displayed visually.

### When to skip

- **Content that's already visual** (screen recordings, product demos) â€” the screen IS the graphic.
- **Videos under 20s** â€” an infographic overlay would dominate too much of the runtime.
- **No clear concept** â€” if the video is conversational/anecdotal without a structured concept, forcing an infographic feels artificial.

### QC

- **Dry-run review:** Inspect the preview image. Is the text readable at phone size? Does the layout make sense? Is the concept accurate to what the speaker says?
- **Timing check:** Does the start_time align with when the speaker begins discussing this concept? The infographic should reinforce, not precede, the spoken explanation.
- **After render:** Scrub to the infographic timestamp and verify it appears/disappears cleanly without clipping other overlays.

### B-roll inserts (`add_broll.py`)

Adds 3 Pexels-sourced b-roll clips (3 seconds each) at LLM-chosen timestamps. Main video audio continues over the b-roll so dialogue stays in sync. Output is ~9s longer.

**Env:** `PEXELS_API_KEY` (required), `OPENROUTER_API_KEY` (for LLM).

```bash
python -m src.edit.add_broll <video.mp4> <output.mp4> --transcript <segment>-words.json
```

- `--dry-run` prints the LLM plan (3 search queries, 3 insert times) and checks Pexels for each query; no download or encode.
- Transcript should match the video (e.g. use `07_music-words.json` for the final segment file). LLM suggests queries and timestamps from the transcript; insert times are clamped to avoid first/last 5s.

### Short-version trim (35 Â± 10s)

Create a **shorter cut** of each segmentâ€™s final infographic video for platforms that favor ~35s. Use **target 35s, tolerance 10s** (acceptable range 25â€“45s). Do **not** use 30s; 35 Â± 10 is the spec.

- **Input:** `09_infographic.mp4` (and optionally `07_music-words.json` or the infographic videoâ€™s word transcript).
- **Output:** e.g. `09_infographic_35s.mp4` (or keep a single convention like `09_infographic_short.mp4`).

```bash
# One segment (from repo root). Script auto-transcribes if --transcript omitted.
dotenvx run -f .env -f .env.dev -- uv run python -m src.edit.trim_smart \
  "<outputs_dir>/segment_XX/09_infographic.mp4" \
  "<outputs_dir>/segment_XX/09_infographic_35s.mp4" \
  --duration 35 --tolerance 10
```

- **Target:** 35 seconds. **Tolerance:** Â±10s â†’ range 25â€“45s.
- trim_smart uses the LLM to pick sections (hook, body, **closer/CTA**, no repeated idea). Pass `--transcript <segment>-words.json` if you have a word-level transcript that matches the video; otherwise the script transcribes the input.
- **Include the infographic:** add_infographic writes a plan sidecar `09_infographic.plan.json` (start_time, end_time) next to each output. When creating the short version, pass `--include-range START END` so the trim is forced to include that range and the short still shows the infographic. The batch script `trim_short_versions_all_segments.sh` auto-reads the plan and passes `--include-range` when present. If you have old segments without a plan file, re-run add_infographic for those segments so the plan is written, then re-run the short-version trim.
- Re-run for every segment that should have a short version; use a loop or `scripts/trim_short_versions_all_segments.sh` (see below).

---

## Phase 7: Background Music (optional â€” only when requested)

**Skip this step by default.** Only add background music if the user explicitly asks for it.

**Enhance voice again before adding music.** Run `enhance_voice` on the captioned clip so the voice cuts through the music better. This produces `06_enhanced.mp4`; then add music from that file.

```bash
# Per segment: enhance captioned clip, then add music
python -m src.edit.enhance_voice <segment_XX/06_captioned.mp4> <segment_XX/06_enhanced.mp4>
python -m src.edit.add_background_music <segment_XX/06_enhanced.mp4> ... <segment_XX/07_music.mp4>
```

**Use the client's approved music library** at `<client>/editing/music/`. Each client has a `preferences.md` describing their music vibe, hard rules (e.g. no vocals, no EDM), and a list of pre-downloaded approved tracks.

**Single track** (same music for every segment):

```bash
python -m src.edit.add_background_music <06_enhanced.mp4> <client>/editing/music/<track>.mp3 <final.mp4>
```

**Round-robin** (cycle through multiple tracks by segment number). Pass the **music directory** and `--segment <N>` (1-based). Tracks are chosen in sorted filename order; segment N uses track at index `(N - 1) % num_tracks`:

```bash
# One segment (e.g. segment 4):
python -m src.edit.enhance_voice <segment_04/06_captioned.mp4> <segment_04/06_enhanced.mp4>
python -m src.edit.add_background_music <segment_04/06_enhanced.mp4> <client>/editing/music/ <segment_04/07_music.mp4> --segment 4

# All segments: enhance then add music (from repo root)
OUT=projects/<ClientName>/editing/videos/<session>/outputs
MUSIC_DIR=projects/<ClientName>/editing/music
for seg in 01 02 03 04 05 06 07 08 09 10; do
  n=${seg#0}
  dotenvx run -f .env -f ../fast-backend/.env -- uv run python -m src.edit.enhance_voice "$OUT/segment_$seg/06_captioned.mp4" "$OUT/segment_$seg/06_enhanced.mp4"
  dotenvx run -f .env -f ../fast-backend/.env -- uv run python -m src.edit.add_background_music "$OUT/segment_$seg/06_enhanced.mp4" "$MUSIC_DIR" "$OUT/segment_$seg/07_music.mp4" --segment "$n"
done
```

- Voice is normalized to -16 LUFS, music to -35 LUFS (voice stays dominant).
- Music starts 5 seconds into the track to skip intros.
- **Single track:** reuse the same file for all segments for consistency. **Multiple tracks:** use round-robin so each segment gets a different track in rotation (e.g. 5 tracks â†’ segments 1â€“5 get 01â€“05, segments 6â€“10 repeat 01â€“05).
- If the client has no `editing/music/` folder yet, use `--tags` to search Openverse, present options to the user, then download approved tracks into the folder. Check `preferences.md` (and `docs/findmusic.md` for profileâ†’music mapping) for guidelines on what to search for.

---

## Phase 8: Generate Copy (Title + Caption)

```bash
# For a short-form video post:
python src/edit/write_copy.py --transcript <segment-words.json> --voice <client>/editing/voices/marky_default.md --platform short

# For LinkedIn (text-only, 3 options):
python src/edit/write_copy.py --transcript <words.json> --voice <client>/editing/voices/marky_default.md --platform linkedin --count 3

# For Twitter (text-only, 5 options):
python src/edit/write_copy.py --transcript <words.json> --voice <client>/editing/voices/marky_default.md --platform twitter --count 5

# For a carousel caption:
python src/edit/write_copy.py --transcript <words.json> --voice <client>/editing/voices/marky_default.md --platform carousel
```

- Reads transcript + voice file and generates a `{"title": "...", "caption": "..."}` JSON.
- `--platform` options: `short`, `linkedin`, `twitter`, `facebook`, `carousel`.
- `--count N` generates N distinct options (each takes a different angle).
- Outputs JSON to stdout â€” capture it or pipe it.
- The LLM follows `<client>/editing/voices/marky_default.md` tone rules: no buzzwords, short sentences, concrete, human.

### Quality control on copy

- Read every generated caption. Does it sound like the founder, or like an AI?
- Titles should be 2-8 words, uppercase-friendly, work as a video overlay.
- LinkedIn posts should end with a genuine question (not "What do you think?").
- Twitter posts must be under 280 characters.
- Each option should take a DIFFERENT angle â€” reject if they're just rephrases.

---

## Phase 9: Schedule Posts

```bash
# Basic â€” single caption for all platforms.
# Omit --business_id when the video is under projects/<Client>/editing/videos/...;
# schedule_post reads business_id from that project's editing/settings.json.
python src/edit/schedule_post.py \
  --video <final.mp4> \
  --title "Title Here" \
  --caption "Base caption text"

# With per-platform caption overrides (inline):
python src/edit/schedule_post.py \
  --video <final.mp4> \
  --title "Title Here" \
  --caption "Base caption (used for IG/TikTok/YT)" \
  --override twitter "Short punchy tweet under 280 chars" \
  --override linkedIn "Longer LinkedIn thought leadership post..."

# With overrides from files (@ prefix reads from file â€” useful for long captions):
python src/edit/schedule_post.py \
  --video <final.mp4> \
  --caption "Base caption" \
  --override twitter @twitter_caption.txt \
  --override linkedIn @linkedin_caption.txt
```

- `--status` defaults to `SCHEDULED`. Other options: `NEW`, `LIKED`.
- **Business ID:** Resolved in order: `--business_id` â†’ project `editing/settings.json` (when video path is under `projects/<Client>/editing/videos/...`) â†’ `MARKY_BUSINESS_ID` env var. Put `business_id` in each project's `editing/settings.json` so you don't need to pass it or set the env per run.
- The script uploads to Supabase Storage, creates a `business_media` record, and creates a `post`.
- **No need to pass a scheduled time** â€” the system auto-schedules in a queue.

### Per-platform overrides

`--override PLATFORM CAPTION` creates a row in `post_channel_overrides` â€” one video post, different captions per platform. This is repeatable (use multiple `--override` flags).

Valid platforms: `instagram`, `facebook`, `linkedIn`, `twitter`, `tiktok`, `pinterest`, `googleBusiness`.

**Recommended workflow with `write_copy.py`:**

```bash
# Option A: Batch mode â€” per-platform voice files, one command:
python src/edit/write_copy.py --transcript <words.json> \
  --platforms \
    short:<client>/editing/voices/marky_video.md \
    twitter:<client>/editing/voices/marky_twitter.md \
    linkedin:<client>/editing/voices/marky_linkedin.md \
  > /tmp/copy.json

# Output is keyed JSON: {"short": {"title": "...", "caption": "..."}, "twitter": {...}, ...}

# Then schedule with overrides:
python src/edit/schedule_post.py \
  --video <final.mp4> \
  --title "$(jq -r .short.title /tmp/copy.json)" \
  --caption "$(jq -r .short.caption /tmp/copy.json)" \
  --override twitter "$(jq -r .twitter.caption /tmp/copy.json)" \
  --override linkedIn "$(jq -r .linkedin.caption /tmp/copy.json)"
```

```bash
# Option B: Same voice for all platforms (single --voice):
python src/edit/write_copy.py --transcript <words.json> --voice <client>/editing/voices/marky_default.md --platform short > /tmp/short.json
python src/edit/write_copy.py --transcript <words.json> --voice <client>/editing/voices/marky_default.md --platform twitter > /tmp/twitter.json
python src/edit/write_copy.py --transcript <words.json> --voice <client>/editing/voices/marky_default.md --platform linkedin > /tmp/linkedin.json
```

The base `--caption` is used as the default for any platform without an override. Overrides only replace the caption â€” media (video) is shared across all platforms.

**Answer The People (ATP) for copy:** To optimize captions for real search demand (like the ideate script does for topics), use **`suggest_video_title.py`** â€” it calls the same AnswerThePublic API as `src/ideate/ideate.py`. Run it once on the full or segment transcript, save the output, then pass that file to `write_copy.py` as `--search-context`:

```bash
# 1. Get ATP-backed search queries (RAPID_API_KEY required)
uv run python suggest_video_title.py --transcript <words.json> --save outputs/atp_context.json

# 2. Generate copy using those queries so captions match what people search for
python src/edit/write_copy.py --transcript <words.json> --voice <client>/editing/voices/marky_default.md \
  --platform short --search-context outputs/atp_context.json
```

`--search-context` also accepts a plain JSON array of strings (hand-written phrases) if you donâ€™t run the ATP script.

---

## Phase 10: YouTube Long-Form (Title, Description, Thumbnail)

The source video itself needs to be posted to YouTube with proper packaging. Refer to `<client>/editing/voices/marky_youtube.md` for the full guidelines.

### Title

Use `write_copy.py` to generate options, but the title needs to **pair with the thumbnail** â€” they tell different halves of the same story.

```bash
python src/edit/write_copy.py --transcript <full-words.json> --voice <client>/editing/voices/marky_default.md --platform short --count 3
```

Then manually pick/edit the best one. YouTube titles are 50-70 chars, sentence case or lowercase.

### Description

Use the reusable template pattern (see `<client>/editing/voices/marky_youtube.md`). The only video-specific parts are:
- First line: a hook or CTA link
- Timestamps: generate from the word-level transcript

### Thumbnail (2-step Gemini generation)

Thumbnails can't be extracted from the video â€” they need to be composed. Use a 2-step process:

**Step 1: Generate expressive portrait**
```python
# Feed a reference frame of the speaker to Gemini Pro with an expression prompt
client.models.generate_content(
    model='gemini-3-pro-image-preview',
    contents=[
        reference_image,
        'Generate a close-up portrait of this person with [expression]. '
        'Professional headshot, clean background. Aspect ratio 3:4.',
    ],
    config=types.GenerateContentConfig(
        image_config=types.ImageConfig(aspect_ratio='3:4')
    ),
)
```

**Step 2: Compose full thumbnail**
```python
# Feed the generated portrait + describe the full layout
client.models.generate_content(
    model='gemini-3-pro-image-preview',
    contents=[
        expressive_portrait,
        'Create a YouTube thumbnail (16:9 landscape) using this person. '
        'LEFT: face cutout. RIGHT: [visual element]. '
        'BOLD TEXT: "[2-3 words]". Dark background, high contrast. '
        'Aspect ratio 16:9.',
    ],
    config=types.GenerateContentConfig(
        image_config=types.ImageConfig(aspect_ratio='16:9')
    ),
)
```

### Thumbnail + Title Pairing Rules

- Thumbnail shows the **emotion + visual hook** (face + graph + "PICK ONE")
- Title gives the **context** ("the 2 things every social platform does differently")
- They NEVER repeat each other
- Test: cover one â€” does the other still make you curious?

---

## Phase 12: Text Posts (LinkedIn, Twitter, Facebook)

Use `write_copy.py` to generate text-only posts from the **full video transcript** (not individual segments).

```bash
# LinkedIn: 3 long-form thought leadership posts
python src/edit/write_copy.py --transcript <full-words.json> --voice <client>/editing/voices/marky_default.md --platform linkedin --count 3

# Twitter: 5-10 punchy posts  
python src/edit/write_copy.py --transcript <full-words.json> --voice <client>/editing/voices/marky_default.md --platform twitter --count 10

# Facebook: 3 conversational posts
python src/edit/write_copy.py --transcript <full-words.json> --voice <client>/editing/voices/marky_default.md --platform facebook --count 3
```

Then schedule each with `src/edit/schedule_post.py --caption "..."` (no `--video`). Use `--override` for per-platform captions if the same post goes to multiple channels.

---

## Phase 13: Image Carousels (Instagram)

Use `/tmp/gen_carousel.py` (or create a similar script) to generate carousel slides.

### Key settings

- **Model:** `gemini-3-pro-image-preview` (Banana Pro) â€” NOT `gemini-2.5-flash-image` (Flash has bad text rendering/spelling).
- **Aspect ratio:** `3:4` for Instagram portrait.
- **Parallelism:** Generate all slides concurrently via `ThreadPoolExecutor` â€” cuts wall-clock time from ~160s to ~25s for 8 slides.

### Design principles

- Typography IS the design. No stock photos, no clip art.
- Clean backgrounds (soft gradients, muted tones).
- Vary font sizes, weights, and placement to emphasize key concepts.
- Color palette: warm neutrals (off-white, charcoal, muted gold/terracotta).
- Slide 1: Hook/title slide (make them want to swipe).
- Last slide: CTA â€” "Save this. Share it." + correct handle (`@mymarky.ai`).

### Known limitations

- Image generation models WILL misspell words sometimes. Inspect every slide.
- The model may duplicate words or hallucinate handles. Always specify exact text in the prompt.
- For pixel-perfect text, consider generating backgrounds with Gemini and overlaying text with Pillow.

### Scheduling carousels

Upload each slide image to Supabase Storage, then create a single post with all image URLs in the `media_urls` array.

---

## Full Pipeline Summary

```
PRE-PRODUCTION (per client, done once):
  A.  Profiler           Follow src/profiler.md â†’ produce profile.md (stable) + strategy.md (content strategy, active)
  B.  Locate             src/locate/idea_generator.py --profile brand_profile.json â†’ topic ideas

PRODUCTION (per recording session â€” scripts in src/edit/):
  0.  Read settings        cat <project>/settings.json (if it exists â€” pass values as flags below)
  0b. Download source      yt-dlp (if YouTube URL â€” see Phase 0)
  1.  Transcribe           get_transcript.py
  2.  Propose cuts         propose_cuts.py --transcript
  3.  Quality review       (agent reviews: standalone? cold open? single vs multi?)
  3b. Text verification    Phase 2b â€” extract text for each timestamp range, verify sentence boundaries (MANDATORY)
  4.  Cut segments         apply_cuts.py (one per segment; prepend cold open if applicable)
  4b. Speedup to target WPM   apply_speedup.py --wpm 220 (BEFORE jump cuts; use settings.json speedup.target_wpm)
  4c. Jump cuts + dead space apply_jump_cuts.py (auto-transcribes the sped-up video)
  4d. Propose hooks         propose_hooks.py â†’ user records â†’ cut â†’ speedup â†’ trim â†’ compose (OPTIONAL)
  5.  Crop to 9:16         ffmpeg crop (if targeting TikTok/Reels/Shorts â€” BEFORE overlays)
  6.  Check placement      check_placement.py (on the CROPPED version â€” DO NOT SKIP)
  7.  Voice enhancement    enhance_voice.py (ALWAYS â€” podcast-style warmth + compression)
  7b. Voice isolation      voice_isolation.py (OPTIONAL â€” only if audio is noisy)
  8.  Add title            add_title.py --duration N --height N --anchor X (flags from settings.json)
  8b. Re-transcribe        get_transcript.py <titled.mp4> (MUST â€” jump cuts shifted timestamps)
  9.  Add captions         add_subtitles.py --transcript <titled>-words.json --style X --height N --delay N --caps
  9b. Emoji overlays       add_emojis.py --transcript --dry-run first (OPTIONAL â€” talking heads / explainers)
  9c. Infographic overlay  add_infographic.py --transcript --dry-run first (OPTIONAL â€” abstract concepts)
  9d. Final transcript QC  get_transcript.py <final-video> â†’ read full transcript; verify pristine & coherent (MANDATORY â€” see "Final transcript QC" below)
  10. Add music            enhance_voice 06_captioned â†’ 06_enhanced; then add_background_music 06_enhanced â†’ 07_music (use <client>/editing/music/ with --segment N for round-robin)
  11. Generate copy        write_copy.py --platform short/twitter/linkedin (one per platform)
  12. Schedule shorts      schedule_post.py --video --caption --title --override twitter/linkedIn
  13. YouTube long-form    Title + description + thumbnail (see <client>/editing/voices/marky_youtube.md)
  14. Generate text posts  write_copy.py --platform linkedin/twitter/facebook â†’ schedule_post.py
  15. Generate carousel    gen_carousel.py â†’ upload â†’ schedule_post.py
```

---

## Agent Oversight Role

**You are not just a script runner. You are the creative director and quality gate.**

Every script in this pipeline produces output that can fail in subtle ways â€” wrong placement, bad phrasing, thin content, misspellings, overlapping elements. Your job is to **inspect the output of every step** before moving to the next. If something looks wrong, fix it â€” re-run with different parameters, regenerate, or adjust.

**You are responsible for QC. Do not consider a segment (or the full run) finished until you have verified the final video.** That means: **always get the final transcript of the final video** (e.g. `06_captioned.mp4` or `07_*.mp4` if music was added), read it in full, and confirm it is **pristine and makes sense** â€” coherent narrative, no mid-sentence cut-offs, no jumbled clip order, no interviewer on camera when the segment should open with the guest, no stray phrases (e.g. "cool thank you") that don't belong. If the final transcript is wrong or nonsensical, **backtrack**: compare to `outputs/cuts.json` and the source transcript to find which clip or boundary caused it, fix the cuts (or hook choice), and re-run from `apply_cuts` for that segment. Never hand off to the user with a segment that would embarrass them.

### What to inspect at each phase

| Phase | What to check | How to check |
|-------|--------------|--------------|
| Transcribe | Accuracy of words, especially brand names and proper nouns | Read the transcript text file. Note words that need `--replace` later. |
| Propose cuts | Segment count, durations (25-55s), cohesion, hook quality, **opener coherence** | Read the JSON output. Does each segment tell a complete story? **Read the first 5-10 words of each segment aloud â€” they must be a complete, self-contained statement, not a mid-sentence fragment.** Would YOU watch it? |
| **Cut text verification** | **Every section starts at a sentence boundary, ends cleanly, no stray words** | **Run Phase 2b text extraction for every proposed cut. Read the first and last 5 words of each section. This is the #1 failure point for cuts â€” more important than visual QC.** |
| Cut segments | Correct timestamps, no mid-sentence cuts, smooth flow | Play back (or extract frames at cut boundaries) to verify transitions. |
| Jump cuts | First word not clipped, no gaps > 0.4s | Transcribe the output. Check first word starts at >= 0.08s (consonant onset preserved). Check all inter-word gaps. |
| Speedup | Hook AND main segment both at target WPM | Transcribe both independently. Never compose a raw-speed hook with a sped-up segment. |
| Hook compose | No dead gap at join, no clipped words, correct duration | Transcribe 05_hooked.mp4. Check gap between last hook word and first segment word is < 0.15s. Verify duration matches expected (hook + segment). |
| Voice enhancement | Voice sounds warmer and fuller, no distortion or artifacts | Listen to a few seconds. Compare against the raw version â€” should sound noticeably less tinny/echoey. |
| Voice isolation | Clean audio, no artifacts, voice still sounds natural | Listen to a few seconds of the output. |
| Check placement | Reasonable height values for the video's layout | Review the LLM's reasoning. Does it match what you see in the frames? |
| Add title | Title is NOT on the face, NOT clipped by platform chrome, readable | **Run `preview_chrome.py` at tâ‰ˆ1-2s, then view the output image.** Confirm title is in safe zone and not on face. This is the #1 failure point. |
| Add captions | Captions sit on chest (BELOW chin), don't overlap title, words are correct | **Run `preview_chrome.py` at tâ‰ˆ4s and tâ‰ˆ10s on the captioned video, then view both output images.** Confirm captions are below the chin line and not in chrome. If any face overlap, lower `--height` by 5-10% and re-render before proceeding. |
| **Final transcript (per segment)** | **Transcript of the final video is pristine and makes sense** | **Transcribe the final video (e.g. `06_captioned.mp4`) with `get_transcript.py`, then read the full transcript. Check: coherent narrative order, no mid-sentence cut at start/end, **first sentence is the intended hook (no opener with "Also", "So", "And", "But")**, **no duplicate phrase (same line said twice, e.g. hook and body)**; no interviewer when segment should open with guest; no stray off-topic phrases. If anything is wrong, backtrack to `cuts.json` and source transcript, fix the offending clip(s), re-run from `apply_cuts` for that segment. Do not finish until every segment passes.** |
| Emoji overlays | Emojis reinforce spoken content, spaced 5s+ apart, don't overlap captions/title | **Run `--dry-run` first and read each rationale. After render, scrub to each timestamp â€” emoji should appear as the keyword is spoken, not before or after.** |
| Infographic overlay | Concept is accurate, template fits content, timing matches spoken explanation | **Run `--dry-run` first â€” inspect the preview image at phone size. Is the text readable? Does the layout make sense? Does start_time align with when the speaker discusses the concept?** |
| Add music | Voice is dominant, music is subtle, no audio clipping | Listen to the first 10-15 seconds. |
| Caption/title text | Tone matches <client>/editing/voices/marky_default.md, no buzzwords, appropriate for platform | Read every caption. Would the founder actually say this? |
| Carousel slides | No misspellings, no duplicated words, correct handle, consistent style | **View every slide image.** Text rendering is the weakest link. |
| YouTube thumbnail | Face is recognizable, text is â‰¤3 bold words, high contrast, pairs with title | **View the generated image.** 2-step generation (portrait â†’ composition) â€” inspect both. |
| YouTube title | 50-70 chars, creates curiosity, pairs with thumbnail (doesn't repeat it) | Read it alongside the thumbnail. Cover one â€” does the other still hook you? |
| Scheduling | Correct platform targeting, correct media attached, overrides match platforms | Verify the post was created with the right media_urls. Check `post_channel_overrides` rows match intended platforms. |

### Final transcript QC (MANDATORY)

**Do not consider a segment (or the full repurpose run) complete until you have verified the final video's transcript.** This is the agent's responsibility â€” the user should not have to catch cut-off sentences, jumbled clip order, or wrong hooks.

1. **For each segment**, transcribe the **final** video file (e.g. `outputs/segment_XX/06_captioned.mp4`, or `07_*.mp4` if music was added):
   ```bash
   python src/edit/get_transcript.py <path-to-segment>/06_captioned.mp4
   ```
   This writes `<stem>-transcript.txt` (and word/utterance JSON) next to the video.

2. **Read the full transcript** (the `.txt` file). Verify:
   - **Coherent narrative:** The order of sentences makes sense. No jump from topic A to unrelated topic B and back (e.g. "next gen buyers" â†’ "oh there's my video, my Marquee helps me" â†’ "they're going to Instagram").
   - **Clean start / hook at start:** The first sentence (or first 5â€“10 words) is the intended hook or a proper sentence start â€” not a conjunction or transition ("Also", "So", "And", "But") and not a mid-sentence fragment. If the segment opens with "Also, there are..." after the hook, the cuts are wrong: fix the body start timestamp and re-run from apply_cuts for that segment.
   - **No duplicate phrases:** The same sentence or near-identical line must NOT appear twice (e.g. "everyone thinks you need a perfect credit score to buy a house" said in the hook and again in the body). If you hear the same idea twice, the cuts are wrong â€” trim the body start to after the duplicate and re-run from apply_cuts for that segment.
   - **Clean end:** The last sentence is complete (e.g. "buy cars" not "buy"), and there are no stray phrases (e.g. "cool thank you") that don't belong in the short.
   - **No wrong speaker:** If the segment is meant to feature the guest, it shouldn't open with the interviewer on camera saying the hook or the question.

3. **If the transcript is not pristine:** Backtrack to find the cause.
   - Compare the transcript to `outputs/cuts.json` for that segment: which source timestamp ranges were used, and in what order?
   - Cross-check against the **source** transcript (e.g. `outputs/<source-stem>-words.json` or `-transcript.txt`) to see what text each range contains.
   - Typical fixes: trim a clip end (e.g. end before "cool thank you"), extend a clip end (e.g. include "cars" after "buy"), change the cold-open hook to a different timestamp (e.g. guest saying the line instead of interviewer), or remove a clip that was spliced in the wrong place (e.g. testimonial in the middle of a "next gen" hook).
   - After fixing `cuts.json`, re-run the pipeline for **that segment only** from `apply_cuts` through captions (and any later steps you use). Then re-transcribe the new final video and verify again.

4. **Only when every segment's final transcript is pristine and makes sense** should you report the run as complete. Never hand off with a segment that would embarrass the user.

### Frame extraction for visual QC

**Agent instruction: you can view image files.** When running title/caption QC, **you must run the chrome preview yourself and then open/view the generated images** to verify placement. Do not ask the user to inspect â€” do the inspection as part of the pipeline.

**Preferred: use `preview_chrome.py`** so you see platform UI overlays (TikTok/IG/YouTube safe zones) on the frame. That lets you verify title and captions are not clipped by chrome and not on the face.

1. **Run** `preview_chrome.py` at tâ‰ˆ4s and tâ‰ˆ10s for each captioned segment (or at least one representative segment).
2. **View** the output images (read the .jpg files) and confirm: title and captions are inside the green safe zone, not on the speaker's face, and not in the colored platform-chrome areas. If anything fails, fix placement and re-run before proceeding.

```bash
# From the video-editor repo root â€” preview_chrome.py lives at repo root
# Frame with title + captions (tâ‰ˆ4s): extracts frame, draws chrome zones, saves <stem>_chrome.jpg
python preview_chrome.py <captioned.mp4> --time 4 --output <segment>/06_captioned_4s_chrome.jpg

# Captions only (tâ‰ˆ10s)
python preview_chrome.py <captioned.mp4> --time 10 --output <segment>/06_captioned_10s_chrome.jpg
```

Defaults: `--platform all` (TikTok, Instagram Reels, YouTube Shorts zones); output path defaults to `<video_stem>_chrome.jpg` (same dir as video). Use `--platform tiktok` (or `instagram`, `youtube`) to show one platform only.

**Fallback â€” plain frame only:** Use `extract_frame.py` if you only need a raw frame without chrome overlay:

```bash
python src/edit/extract_frame.py <video> --time 1   # title
python src/edit/extract_frame.py <video> --time 4   # title + captions
python src/edit/extract_frame.py <video> --time 10  # captions only
```

**Always extract and view these frames (with chrome overlay when checking placement).** Do not assume the placement is correct just because the script ran without errors.

### Video playback in the IDE (only when user requests)

Cursor can open and play `.mp4` files directly in the editor. This is useful for full QC when frame extraction isn't sufficient (e.g., checking pacing, transitions, dog footage). **Only use this when the user explicitly asks** â€” don't proactively open videos in the IDE. For routine QC, frame extraction is enough.

### Video description for visual content

For videos where visual content matters (not just talking head â€” e.g., pets, product demos, b-roll), use `describe_video.py` to get a per-second visual description before deciding cuts:

```bash
python src/edit/describe_video.py <video.mp4> --prompt "Give a second-by-second description of what's happening visually..."
```

This is critical when audio gaps contain important visual moments (dog being cute, product in use, reactions) that should NOT be cut. **Only run this when the user requests it or when the content is clearly visual-heavy.**

### When to re-run vs. accept

- **Re-run** if: title overlaps face, captions are unreadable, segment is too short/long, carousel slide has misspellings, caption tone is off.
- **Accept** if: minor imperfections that don't affect watchability (e.g., captions slightly overlap chin in split-screen â€” this is an inherent layout constraint, not a fixable bug).
- **Escalate to the user** if: you're unsure about creative direction, the source video has quality issues, or a trade-off requires human judgment (e.g., "should we skip this thin segment or merge it with another?").

---

## Cold Open Hooks

A cold open pulls a compelling soundbite from later in the video and places it at the very beginning, before the natural intro. The viewer hears something intriguing, then the video "rewinds" to the start. This is a standard technique in podcasts, documentaries, and short-form video.

### When to use a cold open

- The video contains a **standout soundbite** â€” a surprising claim, a counterintuitive insight, an emotional moment, or a strong opinion â€” that would make a viewer think "wait, what? I need to hear the rest."
- The natural opening is **weak or generic** (e.g., "Hey everyone, welcome back..."). A cold open bypasses the slow start.
- The content has a **narrative arc** where the payoff is better than the setup. Front-loading a taste of the payoff creates anticipation.

### When NOT to use a cold open

- **How-to / procedural content** (setup guides, tutorials, recipes): There's no dramatic payoff to tease. The value is the information itself, delivered linearly.
- **The speaker's tone is flat and consistent** throughout â€” no peaks to pull from.
- **The opening is already a strong hook** ("I lost $50K doing this one thing"). Don't compete with a good intro.
- **Every segment would get the same cold open.** If the only hookable moment is the intro, don't reuse it across all segments.

### How to implement

1. **Find the soundbite.** Read the transcript. Look for moments that are surprising, emotional, or create an open question. Good triggers: strong opinions, unexpected numbers, personal stories, "here's what nobody tells you" moments.
2. **Extract 3-8 seconds** of the soundbite. It should be a complete phrase but leave the viewer wanting context.
3. **Prepend it to the segment** using `apply_cuts.py`. Put the soundbite timestamps first in the `--cuts` list, then the rest of the segment follows.
4. **Optional: add a visual cue** (flash to white, brief text overlay like "WAIT FOR IT") at the transition point between the cold open and the real start. This signals the "rewind" to the viewer.

### Examples of good cold open candidates

- "We spent $200K on ads and the thing that actually worked cost us nothing."
- "This is the part nobody warns you about."
- "I almost didn't share this because it's embarrassing."
- A genuine laugh, a visible reaction, an emotional beat.

### Examples of bad cold open candidates

- "Today I'm gonna show you how to..." (this is just the intro)
- Generic product descriptions or feature lists
- Anything that only makes sense with prior context

---

## Decision Points for the Agent

These are places where intelligent judgment is needed (not just running scripts):

1. **How many segments to cut?** Fewer, higher-quality segments beat many thin ones. If a topic doesn't have enough substance for a 30-40s standalone short, skip it.

2. **Title placement:** Always run `check_placement.py`. The same `--height` value does NOT work across different video layouts.

3. **Caption placement:** Use the `CAPTION_HEIGHT_PERCENT` from `check_placement.py`. Split-screen layouts need different values than full-screen.

4. **Word replacements:** Check the transcript for common misheard words (brand names, proper nouns). Add them to `--replace`.

5. **Carousel quality:** Inspect generated slides for misspellings, duplicated words, and hallucinated text. Regenerate individual slides if needed.

6. **Caption writing:** Each post should have a unique caption tailored to the segment's content and the target platform. Don't use the same caption across platforms.

7. **Music selection:** Pick music that matches the energy of the content. Lofi works for most talking-head content. Reuse the same track across segments from the same source for cohesion. Music is optional â€” skip it if the content doesn't benefit from it.

8. **Cold open hook:** Before finalizing cuts, read the transcript for a soundbite that would hook a cold viewer. If one exists, prepend it to the segment. See the "Cold Open Hooks" section for criteria.

9. **Single vs. multi-segment:** Not every video should be split into multiple shorts. Linear procedures, short source videos (<5 min), and content without distinct sub-topics are better as a single condensed short or posted in full.

10. **Outro handling (short-form vs. long-form):** Most source videos have an outro ("thanks for watching, follow me on socials, see you next time"). For **short-form**, exclude long wind-down outros (15-20s "thanks for sticking around" kills retention). However, a **punchy ~3s CTA** ("follow me for more" / "follow me at [handle]") is fine â€” it's short enough that viewers don't swipe, and it gives a clear call to action. Extract just the CTA portion from the source, speed it up to match the main video's pace, and concat. Skip captions on the outro â€” it's too short to matter and avoids re-running the subtitle pipeline. For **YouTube long-form**, keep the full outro â€” it matches the format's pacing and gives viewers a clear exit point.

---

## Common Pitfalls

| Pitfall | Prevention |
|---------|-----------|
| Title on face (split-screen) | Run `check_placement.py` per video |
| Title clipped by platform chrome | Never go below `--height 8` with `--anchor bottom` |
| Captions say "Markey" or "Marquee" instead of "Marky" | Add to `settings.replacements` (e.g. `{"Markey": "Marky", "Marquee": "Marky"}`) and pass `--settings <path>`, or use `--replace "Markey:Marky,Marquee:Marky"` |
| Caption timing drift / out-of-sync captions | **Re-transcribe after any step that changes the audio timeline** (jump cuts, compose, trim). The caption transcript must come from the video being captioned (or an audio-identical ancestor like the pre-title version). A transcript from before jump cuts will be out of sync because dead space removal shifts all timestamps. |
| Carousel text misspellings | Use `gemini-3-pro-image-preview`, inspect all slides |
| Wrong social handle in carousel CTA | Hardcode `@mymarky.ai` in the prompt |
| Music too loud / voice too quiet | Script handles this automatically (-16 / -35 LUFS) |
| Title shown too long | Use `--duration 6` for shorts (not the default 20) |
| Too many thin segments | Prefer quality over quantity; merge related sub-topics |
| Overlays burned before cropping | Always crop to 9:16 BEFORE adding titles/captions |
| Splitting a linear procedure into fragments | Ask: would a cold viewer understand segment 2 alone? If not, make one condensed short |
| Missing cold open opportunity | Read the transcript for standout soundbites before finalizing cuts |
| Hook not sped up before compose | **Always** `apply_speedup.py --wpm 220` (or settings target) on the hook before composing. Pacing mismatch is jarring. |
| First word clipped after compose | `apply_jump_cuts.py` pads 0.12s before first word for consonant onset. If original cut has < 0.12s lead-in, re-cut from source with 0.2s buffer. |
| Dead gap at hook-to-segment join | Trim hook trailing silence to `last_word_end + 0.06s`. Always transcribe to find actual end. Target: total gap at join 0.15s or less. |
| Framerate mismatch in concat | Never stream-copy concat clips with different framerates. Always use `filter_complex` with `fps=30`. Stream copy produces wrong durations silently. |
| Long outro in short-form | Exclude 15-20s wind-down outros from shorts. A punchy ~3s CTA ("follow me for more") is fine â€” extract, speedup to match, concat. Skip captions on the outro. |
| Cut starts at exact word boundary | Always add 0.15-0.2s buffer before the first word in `--cuts`. Plosives start before the transcription timestamp and get clipped without margin. |
| Jump cuts with stale or mismatched transcript | Let the script auto-transcribe (omit `--transcript` and `--speedup`). Passing the full original transcript against a cut/sped-up video requires complex timestamp remapping and is error-prone. Auto-transcription costs ~$0.01 for shorts and gives timestamps that match the actual video. |
| Double jump at content-cut boundaries | `apply_jump_cuts.py` auto-discovers `*.boundaries.json` from `apply_cuts.py` and suppresses zoom toggles near join points. If you see double jumps, check that the boundaries file exists. |
| Video ends abruptly on last frame | `apply_jump_cuts.py` pads 0.25s after the last word. Also ensure `apply_cuts` extends the last cut ~0.3s past the final word end so there's source material for the pad. |
| CTA/ending removed by jump cuts | Let `apply_jump_cuts.py` auto-transcribe (omit `--transcript`). If you must pass a transcript, ensure it matches the actual input video. A stale transcript from before re-cutting will cause new segments to be treated as silence and trimmed. |
| Captions overlapping speaker's face | **Never use `--height 43` for talking heads.** Use `--height 30` (chest level). Always extract frames and verify captions sit BELOW the chin line. If they touch face/chin/jaw, lower by 5-10% and re-render. |
| Segment starts mid-sentence or with "Also"/"So"/"And" | **Read the first 5-10 words of every segment (and of every section, e.g. body after a hook) aloud.** If they're a fragment or start with a continuation word ("Also", "So", "And", "But"), the cut is wrong. Either extend/move the cut so the opener is a full sentence start, or advance the section start past the conjunction. `propose_cuts.py` enforces this in the prompt and in boundary verification (auto-advances body sections that start with continuation words); Phase 2b and Final transcript QC must still verify. |
| Same line said twice (hook + body duplicate) | **Read the full segment transcript (all sections).** If the same sentence or near-identical phrase appears in two sections (e.g. hook and body both say "everyone thinks you need a perfect credit score to buy a house"), the cuts are wrong. Trim the body start to after the duplicate so the viewer only hears the idea once. `propose_cuts.py` now detects duplicate 6+ word phrases across sections and emits a warning; Phase 2b and Final transcript QC must still verify. |
| Cuts proposed without text verification | **Always run Phase 2b (cut text verification) before Phase 3.** Extract the actual transcript text for each proposed timestamp range and read it. This is the single most important QC step â€” everything downstream (titles, captions, music) is wasted if the underlying cuts start/end mid-sentence. The LLM picks timestamps from word-level data and frequently lands on chunk boundaries instead of sentence boundaries. Never assume timestamps are correct without reading the text. |
| Section ends on trailing conjunction | Trim the end timestamp to before stray "and", "but", "so" at the tail of a section. These trailing words are the start of the next sentence, not the end of the current one. Use `text_for_range()` to spot them. |
| Final segment has wrong content / jumbled narrative / cut-off / stray phrases / same line said twice | **Always run Final transcript QC:** transcribe each `06_captioned.mp4` (or final output), read the full transcript, and fix before handing off. If the transcript is wrong (e.g. clip order jumbled, interviewer on camera instead of guest, "cool thank you" in the middle, last word cut off, **or the same sentence/phrase said twice** â€” e.g. "everyone thinks you need a perfect credit score" in hook and body), backtrack: compare to `cuts.json` and source transcript, fix the offending clip(s) or hook (trim body start to remove duplicate), re-run from `apply_cuts` for that segment. Do not consider the run done until every segment's final transcript is pristine. |